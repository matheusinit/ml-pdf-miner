{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajxqeWPm-QWE"
   },
   "source": [
    "# Instalar dependências necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKWsHLsz-O_k"
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pdf_folder = '../data'\n",
    "\n",
    "pdf_files_path = []\n",
    "for root, dirs, files in os.walk(pdf_folder):\n",
    "  for file in files:\n",
    "    if file.endswith('.pdf'):\n",
    "      pdf_files_path.append(os.path.join(root, file))\n",
    "\n",
    "raw_texts = []\n",
    "\n",
    "for pdf_file_path in pdf_files_path:\n",
    "  whole_text = ''\n",
    "\n",
    "  with pdfplumber.open(pdf_file_path) as pdf:\n",
    "    for index, page in enumerate(pdf.pages):\n",
    "      text = page.extract_text()\n",
    "      if index > 0:\n",
    "        whole_text += f\"page: {index} {text}\\n\"\n",
    "      else:\n",
    "        whole_text += f\"{text}\\n\"\n",
    "\n",
    "  raw_texts.append(whole_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrair o texto não estruturado do PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYhpBB4oDlA_"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data={'raw': raw_texts})\n",
    "\n",
    "row_count = dataframe.count()\n",
    "columns = dataframe.columns\n",
    "print(f\"Row count: {row_count}\")\n",
    "print(f\"Columns: {columns}\")\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_based_on_appearance_in_text(text_to_extract: str, text: str, type: str = 'after'):\n",
    "  \"\"\"\n",
    "  Extracts the text after the appearance of a specific text in a given string.\n",
    "\n",
    "  Parameters:\n",
    "    text_to_extract (str): The text to extract.\n",
    "    text (str): The input string.\n",
    "    type (str): The type of extraction. It can be either \"before\" or \"after\".\n",
    "\n",
    "  Returns:\n",
    "    str: The extracted text.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the type is neither \"before\" nor \"after\".\n",
    "  \"\"\"\n",
    "\n",
    "  import re\n",
    "\n",
    "  if type == 'before':\n",
    "    search = re.search(rf'.*(?<={text_to_extract})', text, re.DOTALL)\n",
    "  elif type == 'after':\n",
    "    search = re.search(rf'(?<={text_to_extract}).*', text)\n",
    "  else:\n",
    "    raise ValueError('type must be either \"before\" or \"after\"')\n",
    "\n",
    "  extracted_text = search.group().strip() if search else None\n",
    "\n",
    "  return extracted_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all essential data from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "def extract_needed_information_from_pdf_text(text: str):\n",
    "  \"\"\"\n",
    "  Extracts the needed information from a given PDF text.\n",
    "\n",
    "  Parameters:\n",
    "    text (str): The input text.\n",
    "\n",
    "  Returns:\n",
    "    dict: The extracted information.\n",
    "  \"\"\"\n",
    "\n",
    "  # Remove redundant texts that are not useful for the analysis\n",
    "  text = re.sub(r'\\bTribunal Regional Eleitoral do Rio Grande do Norte\\n\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\bPJe - Processo Judicial Eletrônico\\n\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\bN úmero: \\b', '', text, count=1)\n",
    "  # Extracting the date of process\n",
    "  date_pattern = r'\\d{2}/\\d{2}/\\d{4}'\n",
    "  date_match = re.search(date_pattern, text)\n",
    "  date = date_match.group() if date_match else None \n",
    "  print(date)\n",
    "\n",
    "  text = re.sub(date_pattern, '\\n', text, count=1)\n",
    "\n",
    "  # Extracting the legal action number\n",
    "  legal_action_number_pattern = r'\\b\\d{7}-\\d{2}.\\d{4}.\\d{1}.\\d{2}.\\d{4}\\b' \n",
    "  legal_action_number_match = re.search(legal_action_number_pattern, text)\n",
    "  legal_action_number = legal_action_number_match.group() if legal_action_number_match else None\n",
    "  text = re.sub(legal_action_number_pattern, '\\n', text, count=1)\n",
    "  print(legal_action_number)\n",
    "\n",
    "\n",
    "  # Extracting the data of raw text\n",
    "  search = 'Classe:'\n",
    "  legal_class = extract_text_based_on_appearance_in_text(search, text)\n",
    "  print(legal_class)\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {legal_class}\\n\\b', '', text, count=1)\n",
    "\n",
    "  search = 'Órgão julgador:'\n",
    "  tribunal = extract_text_based_on_appearance_in_text(search, text)\n",
    "  print(tribunal)\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {tribunal}\\n\\b', '', text, count=1)\n",
    "\n",
    "  search = 'Última distribuição :'\n",
    "  last_distribution = extract_text_based_on_appearance_in_text('Última distribuição :', text)\n",
    "  print(last_distribution)\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {last_distribution}\\n\\b', '', text, count=1)\n",
    "\n",
    "  search = 'Valor da causa:'\n",
    "  cause_cost = extract_text_based_on_appearance_in_text('Valor da causa:', text)\n",
    "  print(f'cause cost: {cause_cost}')\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {cause_cost}\\n\\b', '', text, count=1)\n",
    "  text = re.sub(r'R\\$', '', text, count=1)\n",
    "\n",
    "  search = 'Processo referência:'\n",
    "  reference_legal_action = extract_text_based_on_appearance_in_text('Processo referência:', text)\n",
    "  print(reference_legal_action)\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {reference_legal_action}\\n\\b', '', text, count=1)\n",
    "\n",
    "  search = 'Assuntos:'\n",
    "  matters = extract_text_based_on_appearance_in_text('Assuntos:', text)\n",
    "  print(matters)\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {matters}\\n\\b', '', text, count=1)\n",
    "\n",
    "  search = 'Cargo -'\n",
    "  position = extract_text_based_on_appearance_in_text('Cargo -', text)\n",
    "  print(position)\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {position}\\n\\b', '', text, count=1)\n",
    "\n",
    "  legal_action_goal_string = 'Objeto do processo: '\n",
    "  judicial_secrecy = 'Segredo de Justiça?'\n",
    "  extracted_text = re.search(f'{legal_action_goal_string}(.*?){judicial_secrecy}', text, re.DOTALL)\n",
    "  legal_action_goal = extracted_text.group(1).strip() if extracted_text else None\n",
    "  print(legal_action_goal)\n",
    "\n",
    "  text = re.sub(rf'\\b{legal_action_goal_string}\\b', '', text.strip(), count=1)\n",
    "  legal_action_goal = legal_action_goal.replace('\\n', ' ').strip()\n",
    "\n",
    "  legal_action_goal_splitted = legal_action_goal.split(' ')\n",
    "\n",
    "  for word in legal_action_goal_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "\n",
    "  text = text.strip()\n",
    "\n",
    "  search = 'Segredo de Justiça\\?'\n",
    "  judicial_secrecy = extract_text_based_on_appearance_in_text('Segredo de Justiça\\?', text)\n",
    "  print(judicial_secrecy)\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {judicial_secrecy}\\b', '', text, count=1)\n",
    "\n",
    "  search = 'Justiça gratuita\\?'\n",
    "  free_judicial = extract_text_based_on_appearance_in_text('Justiça gratuita\\?', text)\n",
    "  print(free_judicial)\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {free_judicial}\\b', '', text, count=1)\n",
    "\n",
    "  search = 'Pedido de liminar ou antecipação de tutela\\?'\n",
    "  formal_request = extract_text_based_on_appearance_in_text('Pedido de liminar ou antecipação de tutela\\?', text)\n",
    "  print(formal_request)\n",
    "\n",
    "  text = re.sub(rf'\\b{search} {formal_request}\\b', '', text, count=1)\n",
    "\n",
    "  text = re.sub(r'\\bPartes Advogados\\n\\b', '', text, count=1)\n",
    "\n",
    "  search = r'\\(REQUERENTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' '))\n",
    "  requerente = match.group(1).strip() if match else None\n",
    "  print(requerente)\n",
    "\n",
    "  text = re.sub(r'\\b(REQUERENTE)\\b', '', text, count=1)\n",
    "  requerente_splitted = requerente.split(' ') if requerente != None else []\n",
    "  for word in requerente_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(IMPUGNANTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  impugnante = match.group(1).strip() if match else None\n",
    "  print(f'impugnante: {impugnante}')\n",
    "\n",
    "  text = re.sub(r'\\b(IMPUGNANTE)\\b', '', text, count=1)\n",
    "  text = text.strip()\n",
    "  impugnante_splitted = impugnante.split(' ') if impugnante != None else []\n",
    "  for word in impugnante_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(IMPUGNANTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  impugnante_2 = match.group(1).strip() if match else None\n",
    "  print(f'impugnante: {impugnante_2}')\n",
    "\n",
    "  text = re.sub(r'\\b(IMPUGNANTE)\\b', '', text, count=1)\n",
    "  text = text.strip()\n",
    "  impugnante_splitted_2 = impugnante_2.split(' ') if impugnante_2 != None else []\n",
    "  for word in impugnante_splitted_2:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  # print(text)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(IMPUGNADO\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  impugnado = match.group(1).strip() if match else None\n",
    "  print(f\"impugnado: {impugnado}\")\n",
    "\n",
    "  text = re.sub(r'\\b(IMPUGNADO)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{impugnado}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(ADVOGADO\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' '))\n",
    "  advogado = match.group(1).strip() if match else None\n",
    "  print(advogado)\n",
    "\n",
    "  advogado_splitted = advogado.split(' ') if advogado != None else []\n",
    "  for word in advogado_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\b(ADVOGADO)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{advogado}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(IMPUGNADO\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  impugnado_2 = match.group(1).strip() if match else None\n",
    "  print(f\"impugnado 2: {impugnado_2}\")\n",
    "\n",
    "  text = re.sub(r'\\b(IMPUGNADO)\\b', '', text, count=1)\n",
    "  text = text.strip()\n",
    "  impugnado_splitted_2 = impugnado_2.split(' ') if impugnado_2 != None else []\n",
    "  for word in impugnado_splitted_2:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(REQUERENTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  requerente_2 = match.group(1).strip() if match else None\n",
    "  print(requerente_2)\n",
    "  \n",
    "  requerente2_splitted = requerente_2.split(' ') if requerente_2 != None else []\n",
    "  for word in requerente2_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\b(REQUERENTE)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{requerente_2}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(ADVOGADO\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' '))\n",
    "  advogado_2 = match.group(1).strip() if match else None\n",
    "  print(f\"advogado 2: {advogado_2}\")\n",
    "\n",
    "  text = re.sub(r'\\b(ADVOGADO)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{advogado_2}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(REQUERENTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  requerente_3 = match.group(1).strip() if match else None\n",
    "  print(requerente_3)\n",
    "\n",
    "  requerente3_splitted = requerente_3.split(' ') if requerente_3 != None else []\n",
    "  for word in requerente3_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\b(REQUERENTE)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{requerente_3}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(REQUERENTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  requerente_4 = match.group(1).strip() if match else None\n",
    "  print(requerente_4)\n",
    "\n",
    "  requerente4_splitted = requerente_4.split(' ') if requerente_4 != None else []\n",
    "  for word in requerente4_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\b(REQUERENTE)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{requerente_4}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(REQUERENTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  requerente_5 = match.group(1).strip() if match else None\n",
    "  print(requerente_5)\n",
    "\n",
    "  requerente5_splitted = requerente_5.split(' ') if requerente_5 != None else []\n",
    "  for word in requerente5_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\b(REQUERENTE)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{requerente_5}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(REQUERENTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  requerente_6 = match.group(1).strip() if match else None\n",
    "  print(requerente_6)\n",
    "\n",
    "  requerente6_splitted = requerente_6.split(' ') if requerente_6 != None else []\n",
    "  for word in requerente6_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\b(REQUERENTE)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{requerente_6}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(REQUERENTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  requerente_7 = match.group(1).strip() if match else None\n",
    "  print(requerente_7)\n",
    "\n",
    "  requerente7_splitted = requerente_7.split(' ') if requerente_7 != None else []\n",
    "  for word in requerente7_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\b(REQUERENTE)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{requerente_7}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  search = r'\\(REQUERENTE\\)'\n",
    "  match = re.search(rf'(.+?)\\s+{search}', text.replace('\\n', ' ').strip())\n",
    "  requerente_8 = match.group(1).strip() if match else None\n",
    "  print(requerente_8)\n",
    "\n",
    "  requerente8_splitted = requerente_8.split(' ') if requerente_8 != None else []\n",
    "  for word in requerente8_splitted:\n",
    "    word = word.replace('(', r'\\(').replace(')', r'\\)')\n",
    "    text = re.sub(rf'{word}\\n?', '', text, count=1)\n",
    "  text = re.sub(r'\\b(REQUERENTE)\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\b{requerente_8}\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "\n",
    "  outros_participantes_search = 'Outros participantes'\n",
    "  fiscal_de_lei_search = r'\\(FISCAL DA LEI\\)'\n",
    "  extracted_text = re.search(f'{outros_participantes_search}(.*?){fiscal_de_lei_search}', text, re.DOTALL)\n",
    "  fiscal_de_lei_nome = extracted_text.group(1).strip().replace('\\n', ' ') if extracted_text else None\n",
    "  print(fiscal_de_lei_nome)\n",
    "\n",
    "  text = re.sub(rf'\\b{outros_participantes_search}\\n\\b', '', text, count=1)\n",
    "  text = re.sub(rf'\\bFISCAL DA LEI\\b', '', text, count=1)\n",
    "  text = re.sub(r'\\(\\)', '', text, count=1)\n",
    "  text = re.sub(r'\\bPROMOTOR ELEITORAL DO ESTADO DO RIO GRANDE DO\\nNORTE\\b', '', text, count=1)\n",
    "\n",
    "  text = re.sub(r'\\bDocumentos\\b', '', text, count=1)\n",
    "\n",
    "  text_treated_for_index = text.strip().replace('\\n', ' ').split(' ')\n",
    "\n",
    "  print(text_treated_for_index)\n",
    "\n",
    "  text_treated_for_index.remove('Id.')\n",
    "  text_treated_for_index.remove('Data')\n",
    "  text_treated_for_index.remove('da')\n",
    "  text_treated_for_index.remove('Documento')\n",
    "  text_treated_for_index.remove('Tipo')\n",
    "  text_treated_for_index.remove('Assinatura')\n",
    "\n",
    "  print(text_treated_for_index)\n",
    "\n",
    "  id = text_treated_for_index[0]\n",
    "  print(f\"id: {id}\")\n",
    "  \n",
    "  text_treated_for_index.pop(0)\n",
    "\n",
    "  data_da_assinatura = text_treated_for_index[0]\n",
    "  print(f\"data: {data_da_assinatura}\")\n",
    "  text_treated_for_index.pop(0)\n",
    "\n",
    "  initial_index = 0\n",
    "\n",
    "  for index, word in enumerate(text_treated_for_index, start=initial_index):\n",
    "    pattern = r'\\d{2}:\\d{2}'\n",
    "    match = re.match(pattern, word)\n",
    "\n",
    "    if match:\n",
    "      hora_da_assinatura = word\n",
    "      break\n",
    "\n",
    "  text_treated_for_index.remove(hora_da_assinatura)\n",
    "  \n",
    "\n",
    "  data_hora_da_assinatura = f'{data_da_assinatura} {hora_da_assinatura}'\n",
    "  data_hora_da_assinatura_timestamp = datetime.datetime.strptime(data_hora_da_assinatura, '%d/%m/%Y %H:%M').isoformat()\n",
    "  print(data_hora_da_assinatura_timestamp)\n",
    "\n",
    "  tipo = ''\n",
    "\n",
    "  for index, word in enumerate(text_treated_for_index):\n",
    "    if word == 'Sentença' or word == 'Petição':\n",
    "      tipo = word\n",
    "      break\n",
    "    elif word == 'Outros' and text_treated_for_index[index + 1] == 'documentos':\n",
    "      tipo = f'{word} {text_treated_for_index[index + 1]}'\n",
    "      break\n",
    "    elif word == 'Parecer' and text_treated_for_index[index + 1] == 'da' and text_treated_for_index[index + 1] == 'Procuradoria':\n",
    "      tipo = f'{word} {text_treated_for_index[index + 1]} {text_treated_for_index[index + 2]}'\n",
    "      break\n",
    "    elif word == 'Cota' and text_treated_for_index[index + 1] == 'ministerial':\n",
    "      tipo = f'{word} {text_treated_for_index[index + 1]}'\n",
    "      break\n",
    "\n",
    "  print(tipo)\n",
    "  if tipo == 'Sentença' or tipo == 'Petição':\n",
    "    text_treated_for_index.remove(tipo)\n",
    "  elif tipo == 'Outros documentos':\n",
    "    text_treated_for_index.remove('Outros')\n",
    "    text_treated_for_index.remove('documentos')\n",
    "  elif tipo == 'Parecer da Procuradoria':\n",
    "    text_treated_for_index.remove('Parecer')\n",
    "    text_treated_for_index.remove('da')\n",
    "    text_treated_for_index.remove('Procuradoria')\n",
    "  elif tipo == 'Cota ministerial':\n",
    "    text_treated_for_index.remove('Cota')\n",
    "    text_treated_for_index.remove('ministerial')\n",
    "\n",
    "  index = text_treated_for_index.index('page:')\n",
    "  \n",
    "  documento = ' '.join(text_treated_for_index[:index])\n",
    "  print(documento)\n",
    "\n",
    "  resultado = 'NÃO DEFINIDO'\n",
    "\n",
    "  detalhamento = ' '.join(text_treated_for_index[index+2:])\n",
    "  print(detalhamento)\n",
    "  \n",
    "  indeferimento_word_appearance = detalhamento.lower().find('indefiro')\n",
    "\n",
    "  if detalhamento.lower().find('defiro') != -1:\n",
    "    resultado = 'DEFERIDO'\n",
    "  elif detalhamento.lower().find('deferimento') != -1:\n",
    "    resultado = 'DEFERIDO'\n",
    "  elif detalhamento.lower().find('manifesta-se pelo deferimento') != -1:\n",
    "    resultado = 'DEFERIDO'\n",
    "  elif detalhamento.lower().find('homologo') != -1:\n",
    "    resultado = 'DEFERIDO'\n",
    "\n",
    "\n",
    "  if indeferimento_word_appearance != -1:\n",
    "    resultado = 'INDEFERIDO'\n",
    "  elif detalhamento.lower().find('indeferindo-se') != -1:\n",
    "    resultado = 'INDEFERIDO'\n",
    "  elif detalhamento.lower().find('indeferimento') != -1:\n",
    "    resultado = 'INDEFERIDO'\n",
    "\n",
    "  \n",
    "  print(resultado)\n",
    "\n",
    "  data = {\n",
    "    'advogado': advogado,\n",
    "    'advogado_2': advogado_2,\n",
    "    'data_hora_da_assinatura_timestamp': data_hora_da_assinatura_timestamp,\n",
    "    'documento': documento,\n",
    "    'detalhamento': detalhamento,\n",
    "    'fiscal_de_lei_nome': fiscal_de_lei_nome,\n",
    "    'formal_request': formal_request,\n",
    "    'free_judicial': free_judicial,\n",
    "    'id': id,\n",
    "    'judicial_secrecy': judicial_secrecy,\n",
    "    'last_distribution': last_distribution,\n",
    "    'legal_action_goal': legal_action_goal,\n",
    "    'legal_action_number': legal_action_number,\n",
    "    'legal_class': legal_class,\n",
    "    'matters': matters,\n",
    "    'position': position,\n",
    "    'reference_legal_action': reference_legal_action,\n",
    "    'requerente': requerente,\n",
    "    'requerente_2': requerente_2,\n",
    "    'requerente_3': requerente_3,\n",
    "    'requerente_4': requerente_4,\n",
    "    'requerente_5': requerente_5,\n",
    "    'requerente_6': requerente_6,\n",
    "    'requerente_7': requerente_7,\n",
    "    'requerente_8': requerente_8,\n",
    "    'impugnante': impugnante,\n",
    "    'impugnante_2': impugnante_2,\n",
    "    'impugnado': impugnado,\n",
    "    'impugnado_2': impugnado_2,\n",
    "    'resultado': resultado,\n",
    "  }\n",
    "\n",
    "  return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "pdf_files_path_with_error = []\n",
    "\n",
    "for index, raw_text in enumerate(dataframe['raw']):\n",
    "  try:\n",
    "    data = extract_needed_information_from_pdf_text(raw_text)\n",
    "    new_row = pd.DataFrame([data])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "  except ValueError:\n",
    "    pdf_files_path_with_error.append(pdf_files_path[index])\n",
    "    continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_files_path_with_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_defined = df[df.resultado == 'NÃO DEFINIDO']\n",
    "not_defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorizar documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessário para a NLTK\n",
    "nltk.download('stopwords')  # Baixa a lista de stop words (palavras comuns) para uso no processamento de texto\n",
    "nltk.download('punkt')  # Baixa o tokenizer Punkt, necessário para a tokenização de frases\n",
    "\n",
    "# Carregar o modelo de português para o spaCy\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove pontuação\n",
    "    # []: colchetes são usados para definir uma classe de caracteres.\n",
    "    # ^: quando usado no início de uma classe de caracteres, o ^ nega a classe, ou seja, seleciona tudo que não está na classe.\n",
    "    # \\w: corresponde a qualquer caractere alfanumérico (letras e números, incluindo o caractere de sublinhado _)\n",
    "    # \\s: corresponde a qualquer espaço em branco (espaços, tabulações, quebras de linha).\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove underlines\n",
    "    text = re.sub(r'_+', '', text)\n",
    "\n",
    "    # Remove números\n",
    "    # \\d: corresponde a qualquer dígito (de 0 a 9).\n",
    "    # +: significa “um ou mais” do elemento precedente. Portanto, \\d+ corresponde a uma sequência de um ou mais dígitos consecutivos.\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "  # Obtém a lista de stopwords em português usando o NLTK e as converte para um conjunto para melhorar a eficiência da busca\n",
    "  stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "  # Divide o texto em palavras, remove as stopwords e então junta as palavras restantes de volta em uma string\n",
    "  text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df.resultado != 'NÃO DEFINIDO']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['detalhamento'] = data['detalhamento'].apply(remove_noise)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[1]['detalhamento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['detalhamento'] = data['detalhamento'].apply(remove_stopwords)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['detalhamento'][267]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando o modelo em português do spaCy para tokenização\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# # Passa o texto para o pipeline de processamento do spaCy. O resultado é um objeto doc, que contém as palavras e sentenças tokenizadas, além de outras informações linguísticas.\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Extrair sentenças como tokens\n",
    "# sentence_tokens = [sent.text for sent in doc.sents]\n",
    "\n",
    "# print(\"Tokens:\", sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = [token.text for token in doc]\n",
    "# print('Tokens:', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['detalhamento'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'][267]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming e Lemmatização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stemming'] = data['tokens'].apply(apply_stemming)\n",
    "data['stemming'][267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokeniza a frase em palavras\n",
    "palavras = word_tokenize(frase)\n",
    "\n",
    "# Aplica o stemming a cada palavra\n",
    "stemmed_words = [stemmer.stem(word) for word in palavras]\n",
    "\n",
    "print(\"Frase original:\", frase)\n",
    "print(\"Palavras após Stemming:\", stemmed_words)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemming(doc):\n",
    "  doc = ' '.join(doc)\n",
    "\n",
    "  nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "  doc = nlp(doc)\n",
    "\n",
    "  lemmatized_words = [token.lemma_ for token in doc]\n",
    "  \n",
    "  return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemming'] = data['tokens'].apply(apply_lemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemming'][267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "# Ajustar e transformar os documentos em uma matriz TF-IDF\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(data['lemming'].apply(lambda x: ' '.join(x)))\n",
    "vocab = vectorizer_tfidf.get_feature_names_out()\n",
    "print(\"Representação TF-IDF:\\n\", X_tfidf.toarray())\n",
    "print(\"Vocabulário TF-IDF:\\n\", vocab)\n",
    "\n",
    "print(\"Vocabulário TF-IDF:\", vocab)\n",
    "# Imprime a matriz TF-IDF com rótulos de linha e coluna\n",
    "print(\"Matriz TF-IDF:\")\n",
    "print(\"Documento |\", end=\"    \")\n",
    "for palavra in vocab:\n",
    "    print(palavra, end=\"  \")\n",
    "print()\n",
    "for i, doc in enumerate(X_tfidf.toarray()):\n",
    "    print(f\"Documento {i+1}:\", end=\"   \")\n",
    "    for valor in doc:\n",
    "        print(\"{:.2f}\".format(valor), end=\"    \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
