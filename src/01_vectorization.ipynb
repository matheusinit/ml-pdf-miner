{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorizar documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessário para a NLTK\n",
    "nltk.download('stopwords')  # Baixa a lista de stop words (palavras comuns) para uso no processamento de texto\n",
    "nltk.download('punkt')  # Baixa o tokenizer Punkt, necessário para a tokenização de frases\n",
    "\n",
    "# Carregar o modelo de português para o spaCy\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(text):\n",
    "\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Converte para minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove pontuação\n",
    "    # []: colchetes são usados para definir uma classe de caracteres.\n",
    "    # ^: quando usado no início de uma classe de caracteres, o ^ nega a classe, ou seja, seleciona tudo que não está na classe.\n",
    "    # \\w: corresponde a qualquer caractere alfanumérico (letras e números, incluindo o caractere de sublinhado _)\n",
    "    # \\s: corresponde a qualquer espaço em branco (espaços, tabulações, quebras de linha).\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove underlines\n",
    "    text = re.sub(r'_+', '', text)\n",
    "\n",
    "    # Remove números\n",
    "    # \\d: corresponde a qualquer dígito (de 0 a 9).\n",
    "    # +: significa “um ou mais” do elemento precedente. Portanto, \\d+ corresponde a uma sequência de um ou mais dígitos consecutivos.\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # Obtém a lista de stopwords em português usando o NLTK e as converte para um conjunto para melhorar a eficiência da busca\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "    # Divide o texto em palavras, remove as stopwords e então junta as palavras restantes de volta em uma string\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df.resultado != 'NÃO DEFINIDO']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['detalhamento'] = data['detalhamento'].apply(remove_noise)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[1]['detalhamento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['detalhamento'] = data['detalhamento'].apply(remove_stopwords)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['detalhamento'][210]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "data['tokens'] = data['detalhamento'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'][210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming e Lemmatização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_stemming(tokens):\n",
    "#     stemmer = PorterStemmer()\n",
    "#     stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "#     return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['stemming'] = data['tokens'].apply(apply_stemming)\n",
    "# data['stemming'][267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# # Tokeniza a frase em palavras\n",
    "# palavras = word_tokenize(data['detalhamento'][267])\n",
    "\n",
    "# # Aplica o stemming a cada palavra\n",
    "# stemmed_words = [stemmer.stem(word) for word in palavras]\n",
    "\n",
    "# print(\"Frase original:\", data['detalhamento'][267])\n",
    "# print(\"Palavras após Stemming:\", stemmed_words)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemming(doc):\n",
    "    doc = ' '.join(doc)\n",
    "\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "    doc = nlp(doc)\n",
    "\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_version'] = data['tokens'].apply(apply_lemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['clean_version']\n",
    "y = data['resultado']\n",
    "\n",
    "print(len(x),  len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "x_train_flattened = [item for sublist in x_train for item in sublist]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(x_train_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dtm = vect.transform(x_train_flattened)\n",
    "\n",
    "x_test_flattened = [item for sublist in x_test for item in sublist]\n",
    "\n",
    "x_test_dtm = vect.transform(x_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "\n",
    "tfidf_transformer.fit(x_train_dtm)\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "\n",
    "x_train_tfidf_dense = x_train_tfidf.toarray()\n",
    "\n",
    "print(x_train_tfidf_dense)\n",
    "\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(x_train_flattened)\n",
    "\n",
    "print(tfidf_matrix.toarray())\n",
    "print(vectorizer_tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "# Ajustar e transformar os documentos em uma matriz TF-IDF\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(\n",
    "    data['clean_version'].apply(lambda x: ' '.join(x)))\n",
    "vocab = vectorizer_tfidf.get_feature_names_out()\n",
    "print(\"Representação TF-IDF:\\n\", X_tfidf.toarray())\n",
    "print(\"Vocabulário TF-IDF:\\n\", vocab)\n",
    "\n",
    "print(\"Vocabulário TF-IDF:\", vocab)\n",
    "# Imprime a matriz TF-IDF com rótulos de linha e coluna\n",
    "print(\"Matriz TF-IDF:\")\n",
    "print(\"Documento \", end=\"\")\n",
    "for i, doc in enumerate(X_tfidf.toarray()):\n",
    "    print(f\"Documento {i+1}:\", end=\"   \")\n",
    "    for word, tfidf in zip(vocab, X_tfidf[i].toarray()[0]):\n",
    "        if tfidf == 0:\n",
    "            continue\n",
    "        print(f\"{word}: {tfidf:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetorizar dataframe using TFiDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['detalhamento'] = data['clean_version'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuvem de Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Baixar as stop words se necessário\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('portuguese')\n",
    "\n",
    "# Preencher valores nulos e preparar a coluna 'detalhamento' para o TF-IDF\n",
    "data.fillna('', inplace=True)\n",
    "data['detalhamento'] = data['clean_version'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Criar o vetor TF-IDF usando stop words em português\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(data['detalhamento'])\n",
    "\n",
    "# Obter o vocabulário e as somas dos valores TF-IDF para cada palavra\n",
    "vocab = vectorizer_tfidf.get_feature_names_out()\n",
    "tfidf_sum = tfidf_matrix.sum(axis=0).tolist()[0]  # Soma os TF-IDF ao longo de todos os documentos\n",
    "\n",
    "# Mapear cada palavra para seu valor total de TF-IDF\n",
    "word_scores = dict(zip(vocab, tfidf_sum))\n",
    "\n",
    "# Gerar a nuvem de palavras com os pesos de TF-IDF\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_scores)\n",
    "\n",
    "# Exibir a nuvem de palavras\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfidf = data.drop(columns=['requerente_8', 'lemming', 'stemming', 'tokens', 'pdf_file_path', 'resultado'])\n",
    "columns = data_tfidf.columns\n",
    "\n",
    "for column in columns:\n",
    "    print(f'== {column} ==')\n",
    "    if data_tfidf[column].str.strip().any():\n",
    "        tfidf_matrix = vectorizer_tfidf.fit_transform(data_tfidf[column])\n",
    "\n",
    "        data_tfidf[column] = tfidf_matrix.toarray().mean(axis=1)\n",
    "    else:\n",
    "        data_tfidf[column] = df[column].replace('', 0)\n",
    "\n",
    "data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfidf[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/data_tfidf.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trabalhotre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
